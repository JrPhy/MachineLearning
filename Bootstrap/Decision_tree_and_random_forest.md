決策樹是一種模仿人類決策過程的演算法，其標準很好解釋，但理論很難證明。在資料結構中，決策樹是就是樹+條件。一個節點會分出幾個分支，稱為子節點，分支的上層稱為父節點。\
![img](https://upload.wikimedia.org/wikipedia/commons/f/ff/Decision_tree_model.png)\

## 決策樹算法
由之前的拼接條件可知，$$\ G(x) = \sum_{i=1}^{N}q_{i}(x)g_{i}(x) $$，q<sub>i</sub>(x) 為條件，g<sub>i</sub>(x) 為假設。根據假設數量，樹可以是二元樹或是多元樹

function DecisionTree(data D = {(x<sub>n</sub>, y<sub>n</sub>)}<sub>n=1</sub><sup>N</sup>) 
{
if (達到條件) return 基本假設 g<sub>t</sub>(x) \
else \
1. 學習分支條件 b(x)
2. 把 D 分成 C 分，D<sub>C</sub> = {(x<sub>n</sub>, y<sub>n</sub>):b(x<sub>n</sub>) = c}
3. 建立子樹 G<sub>C</sub> <- DecisionTree(D<sub>C</sub>)
4. 回傳 $$\ G(x) = \sum_{c=1}^{C}[b(x) = c]G_{c}(x) $$

## 2. CART
全名為**分類與回歸樹，Classification And Regression Tree**，是一個有名的決策樹算法，內部使用二元樹然後回傳常數。對於分類來說，會回傳主要的常數。而對於平方誤的回歸來說，會回傳常數的平均。會一直建立二元決策樹，然後計算純度，也就是最小化不純度。

#### 不純度的函數
回歸誤差： 
$$\ impurity(D) = \frac{1}{N}\sum_{n=1}^{N}(y_{n} - \bar{y})^2, \bar{y} = \frac{1}{N}\sum_{n=1}^{N}y_{n} $$

分類誤差：
$$\ impurity(D) = \frac{1}{N}\sum_{n=1}^{N}[y_{n} \neq y*] $$，
y* 為 {y<sub>n</sub>} 中的眾數。

Gini 指標：
$$\ 1 - \sum_{k=1}^{K}(\frac{\sum_{n=1}^{N}[y_{n} = k]}{N})^2 $$

對於所有 y<sub>i</sub> 都一樣，不純度為 0 --> g<sub>i</sub>(x) = y<sub>i</sub>\
對於所有 x<sub>i</sub> 都一樣，沒有決策主幹。CART 就是一個完滿的樹。但是完滿的樹代表 E<sub>in</sub>(G) = 0 若 x<sub>i</sub> 都不同，所以需要正規化。一個方法就是用葉子數量來正則化

$$\ arg min_{所有可能的 G(x)} E_{in}(G) + \lambda \Omega(G) $$

G<sup>(0)</sup> 是完滿的樹，G<sup>(i)</sup> = argmin E<sub>in</sub>(G) 使得 G 是 G<sup>(I-1)</sup> 的唯一節點。

## 2. 特性
有些特性是可以用數值表示，如體重、身高等，此時決策樹的主幹為 b(x) = [x<sub>i</sub> ≤ n] + 1, n ∈ R。\
其他特性如是否發燒、是否疼痛或是否很累，此時決策樹的主幹為 b(x) = [x<sub>i</sub> ≤ n] + 1, S ∈ {發燒、疼痛、累......}。\
如果沒有特性，就可以用其他特性來猜其他缺失特性，舉例來說，若沒有體重資料，就可以用身高來猜體重，所以 CART 可以簡單的解決缺失特性

## 3. 優點
1. 很好解釋
2. 輕易地分多類
3. 輕易分類特性
4. 輕易解決缺失特性
5. 很有效的解決非線性資料
6. 幾乎沒有其他模型可以共享特性，除了其他的決策樹

## 4. 隨機森林
在 BAGging 中，將資料分成 n 分，每份都有一個模型，結合所有模型與藉由平均或投票來減少變異數。在決策樹中用條件來分類。在此可以將這兩種算法的優點結合，此即為隨機森林

隨機森林 = BAGging + 完滿的 CART 決策樹

首先從樣本空間來選出子集，每個資料集被丟進 CART，所以會得到很多棵樹，然後聚合這些資料集並得到模型。\
1. BOOSTRAP 可以很容易平行，可能可以拿到非常發散的模型。
2. 繼承了 CART 的優點
3. 聚合消掉了 CART 的缺點，並減少變異數\
所以建議在 CART 中隨機取子集。這些步驟完全是隨機的，所以叫隨機森林。其他沒被選中的資料就拿來做驗證，其機率為 p = ((N-1)/(N))<sup>2</sup> ，如果 N 夠大，那麼

$$\ lim_{N \rightarrow \infty} p = lim_{N \rightarrow \infty} (1 - \frac{1}{N})^N = lim_{N \rightarrow \infty} (1 + \frac{-1}{N})^N = e^{-1} \approx 0.37 $$

## 5. 特徵選擇
資料可能會存在很多特徵，但是否每個特徵都需要?有些特徵是類似的，會得到相同的資訊，而另一些是與預測無關的，我們不需要這些特徵來浪費計算資源。線性模型可以幫我們選擇特徵，若訓練一個線性模型，那麼可以得到 w，一個ㄈ方法是用向量長度來估算重要性。\
對於線性模型來說，決策樹是一個特徵選擇的算法，如果資料很重要，那麼當資料被汙染時，結果會非常不同。一個方法是使用一些機率分布的雜訊，但也有可能會改變原始資料分布。在此我們使用排列測試，把資料打散。舉例來說第 i 筆資料在排列後可能是在第 j 筆，其重要性為

重要性(i) = 表現(D) - 表現(D<sup>(p)</sup>)

此建議是由隨機森林的作者提出。那麼如何估算表現?在隨機森林中，作者建議使用沒抽中的資料誤差來估算 E<sub>oob</sub>，所以重要性可以寫成

重要性(i) = E<sub>oob</sub>(G) - E<sub>oob</sub><sup>(p)</sup>(G)

只需要 OOB 的資料，若需要非線性轉換，那麼隨機森林是一個適當的方法。
