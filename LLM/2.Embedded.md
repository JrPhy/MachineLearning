## 1. Word Embedded
也稱為詞向量，也就是用其他詞去表示一個詞。在數學上要表示平面上一個點會用座標或向量表示，例如 A(1, 2) 或是 $$\overrightarrow{A} = \hat{x} + 2\hat{y}$$。在 n 維空間就有 n 個基向量， $$\overrightarrow{A} = \sum_{i=1}^{n} a_{i} \hat{i} $$ 。所以我們就可以將某個詞看做數學上的向量，用其他詞去表示看成那些詞為基底向量 (basis)，例如斑馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + ......，當然每個詞在電腦中都是用一個向量表示，裡面就是一些數值，如此一來我們就可以用電腦計算。我們也可以將兩詞拿來做加減，例如獅子 = 斑馬 - 草食 + 肉食，也可以用座標來表示關係，例如 (日本, 東京)，如果出現了 (中國, X)，那經過訓練後電腦應該要輸出 X = 北京。當然基向量越多就越能精準表示某個詞，但是計算量也會因此增加，所以太多基向量也不一定是好事。

## 2. 嵌入向量
當我們把字或詞分開後，在字典中即為基底向量，維度會根據使用者所限定而訂，例如限定維度為 10000，那麼動物的向量就可以表示 (0, 0, 0, ..., 1, 0, ..., 0)，有 9999 個 0 與 1 個 1。斑馬的向量即可表示為 (0, 1, 1, ..., 1, 0, ..., 1)，有 m 個 0 與 n 個 1, m+n = 10000。用來表示這些文字的矩陣稱為嵌入矩陣 (Enbedded matrix)，當然現在都已經有預訓練好的矩陣可以直接使用，否則這種標記需要花非常大量的時間，若要自行訓練矩陣，也可用後面的方法來得到。

## 3. 相似詞
當我們把字或詞向量化後就可以來做更多事，第一件就是判斷兩詞意思是否相近，例如斑馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + 陸地 + ...、河馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + 半陸地 + ...、海馬 = 動物 + 肉食 + 海洋 + ...。越多相同的基向量表示兩詞關係越近，所以就可以利用**內積**來做評比。<斑馬, 河馬> > <斑馬, 海馬>，此即為**餘弦相似(Cosine similarity)**，如果內積後的值越接近 1，表示兩詞意義越相近。

## 4. Word2Vec
Word Embedded 可以把文字給數值化並讓電腦去理解，不過要每個字都這樣去標記會需要很多人工，所以比較好的方法是利用**上下文**做判斷，在學生時代的中英文考試，就常常有閱讀測驗在考，根據上下文，「XX」是什麼意思。所以我們就可以直接將一段文章丟進去模型裡面訓練，然後得到嵌入矩陣，為一種非監督式學習的算法，可以省去大量的時間，但需要蒐集大量的文章來做訓練。Word2Vec 中就有兩種算法。

#### 1. CBOW(Continuous Bag-of-Words Model)
類似考題中的克漏字測驗，就是利用一段文章把某個字挖空，然後從上下文去判斷應該要填入什麼字或詞，例如斑馬是____動物。

| 輸入 |  | 投影層 |  | 輸出 | 
| :--- | :--- | :--- | :--- | :--- |
| 斑馬 | ↘ |  |  | 草食 |
| 是   | -> |  | -> | 哺乳 |
| 動物 | ↗ |  |  | 胎生 |

實務上會給一篇文章，以當前文字為中心去看前後文，至於要看多前後多少文字就由窗口大小決定，其中窗口大小即為 NN 中的隱藏層。此算法會從文章的第一個字開始，用滑動窗口的方式到最後一個字，去生成相鄰矩陣，最後再生成嵌入矩陣，詳細可參考[此部影片](https://www.youtube.com/watch?v=UqRCEmrv1gQ)。
![img](https://github.com/JrPhy/MachineLearning/blob/master/LLM/pic/CBOW.jpg)
![img](https://mccormickml.com/assets/word2vec/training_data.png)\
優點是訓練很快，因為只需要將整個文章跑完一遍即完成一次訓練，但是對於生僻字或是一字多義的處理就不太好

#### 2. skip-gram
與 CBOW 完全相反的操作，是利用一個詞去生成前後的詞，例如_______肉食____。在算法上為 CBOW 的相反計算，一樣會去取一個窗口，然後根據中間文字去更新上下文的權重。

| 輸入 |  | 投影層 |  | 輸出 | 
| :--- | :--- | :--- | :--- | :--- |
|      |  |  | ↗ | 獅子 |
| 肉食 | -> |  | -> | 是 |
|      |  |  | ↘ | 動物 |

此訓練方法較慢，會跟隱藏層大小有關，但訓練效果較好。\
這兩種方法都是選擇一個滑動窗口的方式，窗口取的越長當然越能夠準確判斷，但是計算量也會隨之增加，所以多了 GloVe 與 FastText 兩種改善的算法。

## 5. fastText
架構與 CBOW 類似，只是不再是以單字為單位，而是以每個字元為單位，稱為 n-gram，例如「我要學機器學習」這句話，如果 n = 2 則會變成 w = {我要、要學、學機、機器、器學、學習}，在中文看起來可能還好，但英文就差很多了，還會用 <> 包起每個單字，例如 fastText 使用 n = 3 會變成 w = { <fas, ast, stt, ogl, tte, tex, ext>}。這種方法的好處就是對於英文的自首或字根可以很好的分辨，例如 ##er, ##ly, ##ing, ......，缺點也很明顯，就是記憶體要很大，可以採用 hashmap 的方法來減少記憶體使用跟加快搜尋速度。

## 6. [GloVe (Global Vectors for Word Representation)](https://www.acwing.com/blog/content/58975/)
中文為**全局向量的詞嵌入**，與上述方法不同的在於，GloVe 仍然會利用滑動窗口把整篇文章跑一次，並建立共現矩陣 (Co-occurrence Matrix)，也就是某個字出現在某個字左右邊的次數。考慮以下句子
```
我喜歡數學
我愛科學
我喜歡程式
```
寫成共現矩陣就是

|    | 我 | 喜 | 歡 | 愛 | 數 | 學 | 科 | 程 | 式 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 我 | 0 | 2 | 1 | 1 | 0 | 0 | 1 | 0 | 0 |
| 喜 | 2 | 0 | 2 | 0 | 1 | 0 | 0 | 1 | 0 |
| 歡 | 1 | 2 | 0 | 0 | 2 | 0 | 0 | 2 | 0 |
| 愛 | 1 | 0 | 0 | 0 | 0 | 2 | 2 | 0 | 0 |
| 數 | 0 | 1 | 2 | 0 | 0 | 2 | 0 | 0 | 0 |
| 學 | 0 | 0 | 1 | 2 | 2 | 0 | 1 | 0 | 0 |
| 科 | 1 | 0 | 0 | 2 | 0 | 2 | 0 | 0 | 0 |
| 程 | 0 | 1 | 2 | 0 | 0 | 1 | 0 | 0 | 2 |
| 式 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 |

明顯的缺點就是矩陣非常大且有很多 0，可以使用[奇異值分解](https://github.com/JrPhy/numerical/blob/master/Matrix/SVD_and_Psudo-inverse.md)來做降維。接下來使用以下符號代表文字說明\
$$\ X_{i} = \sum_{j=1}^{N} X_{ij} $$ 表示表示詞 j 詞 i 的上下文中出現的次數\
$$\ P_{ij} = P(j|i)= \frac {X_{ij}}{X_{i}} $$ 表示表示詞 j 詞 i 的上下文中出現的機率\
$$\ ratio_{i,j,k} = \frac {P_{ik}}{P_{jk}} $$ 表示表示詞 k 與詞 i 和詞 j 相關度的比值。

此模型除了看局部也看全局，所以整體效果會比上面所提到的三種都還要好。其中最重要的就是 $$\ ratio_{i,j,k} $$ ，當此值 >> 1 時，代表詞 k 與詞 i 關係相近，反之則是與詞 i 關係相近，若是接近 1 就無法判斷。論文給出一個例子，\
![img](https://imgur.com/266iIzJ.jpg)\
當 k = solid 時，比值為 8.9 (大於1)，代表 solid 與 ice 的意思很近、與 steam 的意思很遠。\
當 k = gas 時，比值為 0.085 (小於1)，代表 gas 與 ice 的意思很遠、與 steam 的意思很近。\
當 k = water 時，比值為 1.36 (接近1)，代表 water 與 ice 和 steam 的意思都很近。\
當 k = fashion 時，比值為 0.96 (接近1)，代表 water 與 ice 和 steam 的意思都很遠。\

|    | i, j 相關 | i, j 不相關 |
| :--- | :--- | :--- |
| i, k 相關 | ~ 1 | >> 1 |
| i, k 不相關 | << 1 | ~ 1 |

所以希望我們的目標函數 $$\ F(w_{i}, w_{j}, \tilde{w_{k}}) \approx \frac {P_{ik}}{P_{jk}}, w_{i} $$ 就是使用滑動窗口建立的詞向量。當然等號右邊已知道意義且為一個數值，但左邊還不知道，但要先把他轉成數值，所以作者先將前兩個向量相減在乘上第三個向量\
$\ F((w_{i} - w_{j})^T \tilde{w_{k}}) = F(w_{i}^T \tilde{w_{k}} - w_{j}^T \tilde{w_{k}}) \approx \frac {P_{ik}}{P_{jk}} $\
上述方程式不太好解，但如果我們取 F = exp，那麼就可以換成兩個 exp 相除，就是一個很方便求解的操作

$\ F(w_{i}^T \tilde{w_{k}} - w_{j}^T \tilde{w}_ {k}) = \frac {exp(w_{i}^T \tilde{w_{k}})}{exp(w_{j}^T \tilde{w_{k}})} \approx \frac {P_{ik}}{P_{jk}} \rightarrow w_{i}^T 	\tilde{w}_ {k} = log P_{ik} = log P(X_{ik}|X_{i}) = log \frac {X_{ik}}{X_{i}} = log X_{ik} - log X_{i}$

這邊作者希望能任意交換單詞和上相文單詞，就做了部分變換的措施
1. $\ F((w_{i} - w_{j})^T \tilde{w_{k}}) = \frac {F(w_{i}^T \tilde{w_{k}})}{F(w_{j}^T \tilde{w_{k}})} $
2. $\ F(w_{i}^T \tilde{w_{k}}) = P_{ik} = \frac {X_{ik}}{X_{i}}, F = exp $
3. $\ w_{i}^T \tilde{w_{k}} = log P_{ik} = log X_{ik} - log X_{i}$

發現右邊多了個 $$\ log X_{i} $$，但因為這跟 k 無關，所以可以當作一個常數，所以可以做以下變換

$\ w_{i}^T \tilde{w_{k}} + b_{i} + \tilde{b_{k}} = log X_{ik} $

這樣的模型作者認為所有共現矩陣的權重都一樣，會帶來一些雜訊，所以就引入一個權重函數 f，所以損失函數為

$\ J = \sum^{V} f(X_{ij}) (w_{i}^T \tilde{w_{k}} + b_{i} + \tilde{b_{k}} + log X_{ik})^2 $

其中 f(0) = 0，f(x) 非遞減，f(x) 要盡量小當 x 很大時。所以最後作者選擇 

$\ f(x) = \frac {x}{x_{max}}^{0.75}, x < x_{max}, f(x) = 1, otherwise $。
