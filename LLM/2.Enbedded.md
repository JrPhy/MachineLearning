## 1. Word Embedded
也稱為詞向量，也就是用其他詞去表示一個詞。在數學上要表示平面上一個點會用座標或向量表示，例如 A(1, 2) 或是 $$\overrightarrow{A} = \hat{x} + 2\hat{y}$$。在 n 維空間就有 n 個基向量， $$\overrightarrow{A} = \sum_{i=1}^{n} a_{i} \hat{i} $$ 。所以我們就可以將某個詞看做數學上的向量，用其他詞去表示看成那些詞為基底向量 (basis)，例如斑馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + ......，當然每個詞在電腦中都是用一個向量表示，裡面就是一些數值，如此一來我們就可以用電腦計算。我們也可以將兩詞拿來做加減，例如獅子 = 斑馬 - 草食 + 肉食，也可以用座標來表示關係，例如 (日本, 東京)，如果出現了 (中國, X)，那經過訓練後電腦應該要輸出 X = 北京。當然基向量越多就越能精準表示某個詞，但是計算量也會因此增加，所以太多基向量也不一定是好事。

## 2. 嵌入向量
當我們把字或詞分開後，在字典中即為基底向量，維度會根據使用者所限定而訂，例如限定維度為 10000，那麼動物的向量就可以表示 (0, 0, 0, ..., 1, 0, ..., 0)，有 9999 個 0 與 1 個 1。斑馬的向量即可表示為 (0, 1, 1, ..., 1, 0, ..., 1)，有 m 個 0 與 n 個 1, m+n = 10000。用來表示這些文字的矩陣稱為嵌入矩陣 (Enbedded matrix)，當然現在都已經有預訓練好的矩陣可以直接使用，否則這種標記需要花非常大量的時間，若要自行訓練矩陣，也可用後面的方法來得到。

## 3. 相似詞
當我們把字或詞向量化後就可以來做更多事，第一件就是判斷兩詞意思是否相近，例如斑馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + 陸地 + ...、河馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + 半陸地 + ...、海馬 = 動物 + 肉食 + 海洋 + ...。越多相同的基向量表示兩詞關係越近，所以就可以利用**內積**來做評比。<斑馬, 河馬> > <斑馬, 海馬>，此即為**餘弦相似(Cosine similarity)**，如果內積後的值越接近 1，表示兩詞意義越相近。

## 4. Word2Vec
Word Embedded 可以把文字給數值化並讓電腦去理解，不過要每個字都這樣去標記會需要很多人工，所以比較好的方法是利用**上下文**做判斷，在學生時代的中英文考試，就常常有閱讀測驗在考，根據上下文，「XX」是什麼意思。所以我們就可以直接將一段文章丟進去模型裡面訓練，然後得到嵌入矩陣，為一種非監督式學習的算法，可以省去大量的時間，但需要蒐集大量的文章來做訓練。Word2Vec 中就有兩種算法。

#### 1. CBOW(Continuous Bag-of-Words Model)
類似考題中的克漏字測驗，就是利用一段文章把某個字挖空，然後從上下文去判斷應該要填入什麼字或詞，例如斑馬是____動物。

| 輸入 |  | 投影層 |  | 輸出 | 
| :--- | :--- | :--- | :--- | :--- |
| 斑馬 | ↘ |  |  | 草食 |
| 是   | -> |  | -> | 哺乳 |
| 動物 | ↗ |  |  | 胎生 |

實務上會給一篇文章，以當前文字為中心去看前後文，至於要看多前後多少文字就由窗口大小決定，其中窗口大小即為 NN 中的隱藏層。此算法會從文章的第一個字開始，用滑動窗口的方式到最後一個字，去生成相鄰矩陣，最後再生成嵌入矩陣，詳細可參考[此部影片](https://www.youtube.com/watch?v=UqRCEmrv1gQ)。
![img](https://github.com/JrPhy/MachineLearning/blob/master/LLM/pic/CBOW.jpg)
![img](https://mccormickml.com/assets/word2vec/training_data.png)\
優點是訓練很快，因為只需要將整個文章跑完一遍即完成一次訓練，但是對於生僻字或是一字多義的處理就不太好

#### 2. skip-gram
與 CBOW 完全相反的操作，是利用一個詞去生成前後的詞，例如_______肉食____。在算法上為 CBOW 的相反計算，一樣會去取一個窗口，然後根據中間文字去更新上下文的權重。

| 輸入 |  | 投影層 |  | 輸出 | 
| :--- | :--- | :--- | :--- | :--- |
|      |  |  | ↗ | 獅子 |
| 肉食 | -> |  | -> | 是 |
|      |  |  | ↘ | 動物 |

此訓練方法較慢，會跟隱藏層大小有關，但訓練效果較好。\
這兩種方法都是選擇一個滑動窗口的方式，窗口取的越長當然越能夠準確判斷，但是計算量也會隨之增加，所以多了 GloVe 與 FastText 兩種改善的算法。

## 5. fastText
架構與 CBOW 類似，只是不再是以單字為單位，而是以每個字元為單位，稱為 n-gram，例如「我要學機器學習」這句話，如果 n = 2 則會變成 w = {我要、要學、學機、機器、器學、學習}，在中文看起來可能還好，但英文就差很多了，還會用 <> 包起每個單字，例如 fastText 使用 n = 3 會變成 w = { <fas, ast, stt, ogl, tte, tex, ext>}。這種方法的好處就是對於英文的自首或字根可以很好的分辨，例如 ##er, ##ly, ##ing, ......，缺點也很明顯，就是記憶體要很大，可以採用 hashmap 的方法來減少記憶體使用跟加快搜尋速度。

## 6. GloVe (Global Vectors for Word Representation)
中文為**全局向量的詞嵌入**，
