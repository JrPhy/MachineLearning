在自然語言中最重要的就是要先將人要說的話讓電腦理解，所以要先將自詞作切割並告訴電腦，切割的方式有很多，在英文的世界會將每個單字用空格分開，所以每個單字都是有自己的意義，當然英文也存在片語這種由多個單字組成的詞，但基本上都能從片語中的單字猜到意思，而中文就不一定了，[連綿詞](https://zh.wikipedia.org/zh-tw/%E8%81%AF%E7%B6%BF%E8%A9%9E)就是一個詞拆開後就沒有意義的字。而英文雖然本身就有空格來做分割，但英文在時態上會有一些差異，例如 run 本身是跑的意思，類似的詞有 ran 過去式，runner 跑者，running 正在跑這三種類似的詞，當然這個現象在中文也存在，不過對於大部分的狀況來說，以每個字為最小單位做切割是一個不錯的方法，但是英文字母數量僅有 26 個，單字卻有上萬個，所以用此種方法所需要的記憶體就非常大。字母或單字區分就是兩個極端，實務上就會選擇兩種混用的方法。

## 1. SubWord Tokenizer
一般來說會先準備一個字典來記錄常用的字，當語句中出現沒有在字典中的詞時，就可以利用拼接的方式放進字典裏面成為一個新的詞，如 BERT 中所使用的 WordPiece，或是 GPT 中使用的 BPE(Byte Pair Encoding)。或者是準備一份很大的字典，經由訓練的方式來將不常用到的字或詞從字典中剔除，稱為 Unigram。

#### 1. Byte Pair Encoding (BPE)
最早由Philip Gage 提出，用來做數據壓縮上。它的原理是將常見連續的兩個符號以另一個符號表達。例如 ababab 中 a 後面很常接著 b 我們就用 c 表達 ab，並得到一個新的序列 ccc，c=ab。英文會有相同的字首搭配不同的字根來表示不同時態或是詞性的用法，且英文的最小單位為字母，就很適合用此算法，步驟如下
1. 先將每個單字依照字母做切分，並在結尾加上 </w> 表示結束
2. 統計每兩個連續出現字符的次數並由高到低做排序
3. 將頻率最高的字符"es"合併，得到新的詞表
4. 重複上面三個步驟直到頻率低於某個值
[以下資料可參考此篇](https://www.zhaokangkang.com/article/6843fe1d-f846-4eae-9fd1-cf10fdfb5d15)，舉例來說
```
low
lower
newest
widest
newest
widest
widest
widest
nice
```
先做第一步後變成
```
['l o w </w>', 
'l o w e r </w>', 
'n e w e s t </w>', 
'w i d e s t </w>', 
'n e w e s t </w>', 
'w i d e s t </w>', 
'w i d e s t </w>', 
'w i d e s t </w>', 
'n i c e </w>']
```
在統計後得到
```
{"es": 6, "st": 6, "t</w>": 6, "wi": 4, "id": 4, "de": 4, "we": 3, "lo": 2, "ow": 2, "ne": 2, "ew": 2, "w</w>": 1, "er": 1, "r</w>": 1, "ni": 1, "ic": 1, "ce": 1, "e</w>": 1}
```
進行合併後得到
```
['l o w </w>', 
'l o w e r </w>', 
'n e w es t </w>', 
'w i d es t </w>',
'n e w es t </w>', 
'w i d es t </w>', 
'w i d es t </w>', 
'w i d es t </w>', 
'n i c e </w>']
```
重複以上步驟，最後可以得到
```
['lo w </w>', 'lo w e r </w>', 'n e w est</w>', 'widest</w>', 'n e w est</w>', 'widest</w>', 'widest</w>', 'widest</w>', 'n i c e </w>']
```

#### 2. WordPiece
類似於 BPE，但是在 WordPiece 一個字會被拆成下列表示
```
cat --> ['c'， '##a'， '##t']
```
WordPiece 會去計算一個分數，選擇提升語言模型機率最大的相鄰子字加入詞表。算法如下
```
score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)
```
最後再根據分數去做合併，詳細的方式可以[參考這篇](https://www.51cto.com/article/779682.html)。

#### 3. Unigram
不同於前面兩種，Unigram 是將從一個非常大的資料集中去除資料，如果語料庫中的第一個單字是 cat，則子字串[['c', 'a', 't'], ['ca', 't'], ['c', 'at'], ['cat']]將被添加到詞彙表中。再來就算各組拆分的字串出現在文句中的機率
```
P('c', 'a', 't') = P(a)P(b)P(c)
P('ca', 't') = P(ca)P(t)
P('c', 'at') = P(ca)P(t)
P('cat') = P(cat)
```
假設 P('ca', 't') 的機率最高，那麼 cat 就會被標記為 'ca' 與 't'。當然也有可能同個自備標記成多個分段。例如 tokenizatio 有可能被拆成 'token', 'iza', 'tion'或 'token', 'ization。

#### 4. SentencePiece
結合了 BPE 與 Unigram，對於有漢字或非全用字母的語言有較好的效果。

## 2. 程式撰寫
HUGGING FACE 中提供了許多的範例與教學可以參考。在此使用 transformers 中的 AutoTokenizer，並使用 bert-base-chinese 模型來做 tokenize 看看會有什麼結果。
```PYTHON
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
sequence = "白日依山盡"
tokens = tokenizer.tokenize(sequence)
print(tokens)
#['白', '日', '依', '山', '盡']
```
可以看到每個中文字都被正確的切割，如果選用非中文模型，就會出現不同結果，在此選用 bert-base-cased 模型
```PYTHON
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
sequence = "白日依山盡"
tokens = tokenizer.tokenize(sequence)
print(tokens)
#['白', '日', '[UNK]', '山', '[UNK]']
```
其中的 [UNK] 代表模型並不知道這個字或詞的意思，如果出現很多次的話就代表模型對此語言還不夠強。而英文訓練資料中的常出現的部分會用 ## 表示
```
sequence = "I've been waiting for a HuggingFace course my whole life."
# 輸出 ['I', "'", 've', 'been', 'waiting', 'for', 'a', 'Hu', '##gging', '##F', '##ace', 'course', 'my', 'whole', 'life', '.']
```
在此例子中 '##gging', '##F', '##ace' 三個部分在模型中很常出現，所以常跟其他的字首合起來組成一個字。這邊使用的模型是 BERT，底層是用 WordPiece 去做訓練，也就是從多次少量資料訓練後，知道哪些部分常出現，其他的特殊標籤還有除了一般的 wordpieces 以外，BERT 裡頭有 5 個特殊 tokens 各司其職，隨後會一一介紹。
```
[CLS]：在做分類任務時其最後一層的 repr. 會被視為整個輸入序列的 repr.
[SEP]：有兩個句子的文本會被串接成一個輸入序列，並在兩句之間插入這個 token 以做區隔
[UNK]：沒出現在 BERT 字典裡頭的字會被這個 token 取代
[PAD]：zero padding 遮罩，將長度不一的輸入序列補齊方便做 batch 運算
[MASK]：未知遮罩，僅在預訓練階段會用到
```

