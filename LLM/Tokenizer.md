在自然語言中最重要的就是要先將人要說的話讓電腦理解，所以要先將自詞作切割並告訴電腦，切割的方式有很多，在英文的世界會將每個單字用空格分開，所以每個單字都是有自己的意義，當然英文也存在片語這種由多個單字組成的詞，但基本上都能從片語中的單字猜到意思，而中文就不一定了，[連綿詞](https://zh.wikipedia.org/zh-tw/%E8%81%AF%E7%B6%BF%E8%A9%9E)就是一個詞拆開後就沒有意義的字。而英文雖然本身就有空格來做分割，但英文在時態上會有一些差異，例如 run 本身是跑的意思，類似的詞有 ran 過去式，runner 跑者，running 正在跑這三種類似的詞，當然這個現象在中文也存在，不過對於大部分的狀況來說，以每個字為最小單位做切割是一個不錯的方法，但是英文字母數量僅有 26 個，單字卻有上萬個，所以用此種方法所需要的記憶體就非常大。字母或單字區分就是兩個極端，實務上就會選擇兩種混用的方法。

## 1. SubWord Tokenizer
一般來說會先準備一個字典來記錄常用的字，當語句中出現沒有在字典中的詞時，就可以利用拼接的方式放進字典裏面成為一個新的詞，如 BERT 中所使用的 WordPiece，或是 GPT 中使用的 BPE(Byte Pair Encoding)。或者是準備一份很大的字典，經由訓練的方式來將不常用到的字或詞從字典中剔除，稱為 Unigram。
#### 1. WordPiece
在中文裡有一種特別的語句，同一句話切在不同的地方會有不同的意思，例如「[下雨天留客天留我不留](https://zh.wikipedia.org/zh-tw/%E4%B8%8B%E9%9B%A8%E5%A4%A9%E7%95%99%E5%AE%A2%E5%A4%A9%E7%95%99%E6%88%91%E4%B8%8D%E7%95%99)」，如果字典是以單字或字母為最小單位，那就可以使用此方法來增加字典中的詞彙量。

## 1. Word2Vec
這些詞與哪些詞有類似的意思，稱為語意相似度，例如
|  | 男人 | 男人 | 男人 | 男人 | 男人 | 男人 | 男人 | 男人 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
|  | 男孩 | 國王 | 爸爸 | 父親 | 皇帝 | 酋長 | 一家之主 | 媽媽 |
|  | 0.859 | 0.923 | 0.965 | 0.943 | 0.924 | 0.835 | 0.735 | 0.268 |

當然此相似度是由每個訓練者去定義，例如有些人認為一家之主多數是女人，那麼相似度就會降低。
