前面講了許多理論，以及要如何得到 K, Q, V 矩陣，幸好現在網路發達，也有許多人願意共享自己的成果。[Hugging Face](https://huggingface.co/learn/nlp-course/zh-TW/chapter1/1?fw=pt)是目前比較有名的開源網站，不論是現在流行的預訓練集，還是目前較多人使用的 Transformer 模型，在上面都可以找到詳細的資料。最主要的就是 Encoder 和 Decoder，Transformer 就是將所有的步驟直接串起來，下圖左邊就是 Encoder，用於做閱讀理解，根據特定的標籤對文章輸入進行分類。右邊是 Decoder，根據輸入的提示生成文章。整個串起來一起用就稱為 Seq2Seq，用來生成文章的摘要或翻譯\
![img](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg)\
當一段文字近來，首先會先去切 token，然後再做 embedded，再來是加入注意力機制，最後就可以輸出。Transformer 將此流程包成了 pipeline 如下圖\
![img](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg)\
所以我們只需要將語句直接丟入 pipeline 中即可。

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
print(classifier(
        [
            "Only those who will risk going too far can definitely find out how far one can go.",
            "除卻巫山不是雲"
        ]
    )
)
# [{'label': 'POSITIVE', 'score': 0.9813832640647888}, {'label': 'NEGATIVE', 'score': 0.9438539743423462}]
```
以上就是使用 pipeline 做語意的情感分析。可以看到這個函數可以直接丟 list 且返回 list，裡面為 dict 的資料結構。pipeline 可以做以下的分析。
```
feature-extraction （特徵萃取）
fill-mask （克漏字）
ner（識別）
question-answering （問答）
sentiment-analysis （情感分析）
summarization （摘要）
text-generation （文章生成）
translation （翻譯）
zero-shot-classification （零訓練樣本分類）
```
實際應用很常從預訓練的資料往下做，transformers 也提供了這模組，只要引入 TFAutoModel 即可，裡面主要提供 PyTorch 與 TensorFlow 的版本，預設是 PyTorch，如果是則會在模組名稱前加上 TF，例如下方的 TFAutoModel。
```python
from transformers import TFAutoModel

tf_model = TFAutoModel.from_pretrained(model_name)
tf_model = TFAutoModel.from_pretrained("xlm-roberta-base", from_pt=True)
# 資料僅有 PyTorch 版
```
PyTorch 與 TensorFlow 的資料結構有所不同，所以前者通常會加上 pt，後者則是 tf。如果是 TF 的模組要使用 pt 的資料，那麼只要在引數多增加個 ```from_pt=True``` 即可，反之則是加個 ```from_tf=True```。
