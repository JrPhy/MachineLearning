前面講了許多理論，以及要如何得到 K, Q, V 矩陣，幸好現在網路發達，也有許多人願意共享自己的成果。[Hugging Face](https://huggingface.co/learn/nlp-course/zh-TW/chapter1/1?fw=pt)是目前比較有名的開源網站，不論是現在流行的預訓練集，還是目前較多人使用的 Transformer 模型，在上面都可以找到詳細的資料。最主要的就是 Encoder 和 Decoder，Transformer 就是將所有的步驟直接串起來，下圖左邊就是 Encoder，用於做閱讀理解，根據特定的標籤對文章輸入進行分類。右邊是 Decoder，根據輸入的提示生成文章。整個串起來一起用就稱為 Seq2Seq，用來生成文章的摘要或翻譯\
![img](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg)\
當一段文字近來，首先會先去切 token，然後再做 embedded，再來是加入注意力機制，最後就可以輸出。Transformer 將此流程包成了 pipeline 如下圖\
![img](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg)\

## 1. AutoTokenizer
安裝完套件之後就可以直接使用，可以來看一下對於中文與英文的輸出
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
sequence = "白日依山盡"
tokens = tokenizer.tokenize(sequence)
print(tokens)
# 輸出 ['白', '日', '[UNK]', '山', '[UNK]']
```
其中 [UNK] 表示模型不知道這個 token 的意思。當然還有其他的表示，例如英文訓練資料中的常出現的部分會用 ## 表示
```
sequence = "I've been waiting for a HuggingFace course my whole life."
# 輸出 ['I', "'", 've', 'been', 'waiting', 'for', 'a', 'Hu', '##gging', '##F', '##ace', 'course', 'my', 'whole', 'life', '.']
```
在此例子中 '##gging', '##F', '##ace' 三個部分在模型中很常出現，所以常跟其他的字首合起來組成一個字。這邊使用的模型是 BERT，底層是用 WordPiece 去做訓練，也就是從多次少量資料訓練後，知道哪些部分常出現，其他的特殊標籤還有

| 名稱 | 說明 |
| :--- | :--- |
| [CLS]	| 用於捕獲整個序列的語義信息 |
| [SEP]	| 區隔句子的前後文 |
| [MASK] | 遮蔽文字字元，僅出現在預訓練階段 |
| [UNK]	| 表示未知字元 |
| [PAD]	| 表示填充字元 |
