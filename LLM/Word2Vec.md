## 1. Word Embedded
也稱為詞向量，也就是用其他詞去表示一個詞。在數學上要表示平面上一個點會用座標或向量表示，例如 A(1, 2) 或是 $$\overrightarrow{A} = \hat{x} + 2\hat{y}$$。在 n 維空間就有 n 個基向量， $$\overrightarrow{A} = \sum_{i=1}^{n} a_{i} \hat{i} $$ 。所以我們就可以將某個詞看做數學上的向量，用其他詞去表示看成那些詞為基向量，例如斑馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + ......，當然每個詞在電腦中都是用一個向量表示，裡面就是一些數值，如此一來我們就可以用電腦計算。我們也可以將兩詞拿來做加減，例如獅子 = 斑馬 - 草食 + 肉食，也可以用座標來表示關係，例如 (日本, 東京)，如果出現了 (中國, X)，那經過訓練後電腦應該要輸出 X = 北京。當然基向量越多就越能精準表示某個詞，但是計算量也會因此增加，所以太多基向量也不一定是好事。

## 2. 相似詞
當我們把字或詞向量化後就可以來做更多事，第一件就是判斷兩詞意思是否相近，例如斑馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + 陸地 + ...、河馬 = 動物 + 哺乳類 + 草食 + 四隻腳 + 非洲 + 半陸地 + ...、海馬 = 動物 + 肉食 + 海洋 + ...。越多相同的基向量表示兩詞關係越近，所以就可以利用**內積**來做評比。<斑馬, 河馬> > <斑馬, 海馬>，此即為**餘弦相似(Cosine similarity)**。

## 3. Word2Vec
Word Embedded 可以把文字給數值化並讓電腦去理解，不過要每個字都這樣去標記會需要很多人工，所以比較好的方法是利用**上下文**做判斷，在學生時代的中英文考試，就常常有閱讀測驗在考，根據上下文，「XX」是什麼意思。所以我們就可以直接將一段文章丟進去模型裡面訓練，Word2Vec 中就有兩種算法。
#### 1. CBOW(Continuous Bag-of-Words Model)
類似考題中的克漏字測驗，就是利用一段文章把某個字挖空，然後從上下文去判斷應該要填入什麼字或詞，例如斑馬是____動物。

| 輸入 |  | 投影層 |  | 輸出 | 
| :--- | :--- | :--- | :--- | :--- |
| 斑馬 | ↘ |  |  | 草食 |
| 是   | -> |  | -> | 哺乳 |
| 動物 | ↗ |  |  | 胎生 |

#### 2. skip-gram
與 CBOW 完全相反的操作，是利用一個詞去生成前後的詞，例如_______肉食____。

| 輸入 |  | 投影層 |  | 輸出 | 
| :--- | :--- | :--- | :--- | :--- |
|      |  |  | ↗ | 獅子 |
| 肉食 | -> |  | -> | 是 |
|      |  |  | ↘ | 動物 |

